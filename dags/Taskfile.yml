version: '2'
#env: source from parent taskfile
#vars: source from parent taskfile
env:
  REMOTE_DAG_FOLDER: /usr/local/airflow

tasks:

  default:
    cmds:
      - echo executed on $DOCKER_REGISTRY
    silent: true
    vars:
      DAG_FILE:
        sh: echo $(pwd)/dags/spark_historical_process_localmode_v3.py

######## raw to master dag tasks #########
  historical_process.deploy:
    cmds:
      - task: single_deploy
        vars:
          FILES: |-
            dags/historical_process.py

  historical_process.undeploy:
    cmds:
      - task: single_undeploy
        vars:
          FILES: |-
            dags/historical_process.py

  historical_process.backfill:
    summary: |
       When run locally, you need to specify environment variables
       eg. KUBE_CONTEXT=ew1p3 NAMESPACE=data-flux ENV=prod task dags:historical_process.backfill
    cmds:
      - |
        kubectl exec deploy/airflow -- airflow backfill "{{.DAG_NAME}}" --start_date "{{.START_DATE}}" --end_date "{{.END_DATE}}" --reset_dagruns -y;
    vars:
      START_DATE:
        '{{default "2020-06-02" .START_DATE}}'
      END_DATE:
        '{{default "2020-06-03" .END_DATE}}'
      DAG_NAME: historical_process

  historical_process.daily.reprocess:
    summary: |
       When run locally, you need to specify environment variables
       eg. KUBE_CONTEXT=ew1p3 NAMESPACE=data-flux ENV=prod task dags:historical_process.daily.reprocess
    cmds:
      - task: :set.k8s.context
      - task: historical_process.deploy
      - task: historical_process.backfill
        vars:
          START_DATE:
            # date 5 days ago in format YEAR-MONTH-DAY
            sh: echo {{now | date_modify "-120h" | date "2006-01-02"}}
          END_DATE:
            # date 4 days ago in format YEAR-MONTH-DAY
            sh: echo {{now | date_modify "-96h" | date "2006-01-02"}}

  ############## combined dags operations #############
  deploy:
    summary: |
      Deploy all dags to airflow, it does not include dags run

      It include lists of dags to deploy
    cmds:
      - task: :set.k8s.context
      - echo $NAMESPACE
      - task: historical_process.deploy

  undeploy:
    summary: |
      Undeploy all dags files and its dependencies from airflow
    cmds:
      - task: :set.k8s.context
      - task: historical_process.undeploy

  single_deploy:
    cmds:
      - >
        {{range $f := .FILES | trim | splitLines -}}
            dir=$(dirname {{$f}})
            kubectl exec deploy/airflow -- mkdir -p $dir
            kubectl -n $NAMESPACE cp $(pwd)/{{$f}} $AIRFLOW_POD_NAME:$REMOTE_DAG_FOLDER/{{$f}}
        {{end}}
    vars:
      FILES: |-
        '{{default "__TO_BE_SET__" .FILES}}'
    env:
      AIRFLOW_POD_NAME:
        sh: kubectl -n $NAMESPACE get pods -o jsonpath="{.items[0].metadata.name}" -l app=airflow || echo "no connection"

  single_undeploy:
    cmds:
      - >
        {{range $f := .FILES | trim | splitLines -}}
            kubectl exec deploy/airflow --  rm -rfv $REMOTE_DAG_FOLDER/{{$f}}
        {{end}}
    vars:
      FILES: |-
       '{{default "__TO_BE_SET__" .FILES}}'
